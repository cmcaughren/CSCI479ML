## Assignment 3 CSCI479 Machine Learning 
# Carolyn McAughren October 21 2022

## Problem Description:
   The data set used in this assignment comes from the Car Evaluation Data Set that is available at:
      [Car Evaluation Data Set](http://archive.ics.uci.edu/ml/datasets/Car+Evaluation) 
   The data set is donated by its creator Marko Bohanec at 1997.

   The data set used in this assignment is stored in the csv file CarData.csv. 
   There are 1728 instances in this data set.

   There are 6 descriptive attributes in this data set:

   BUY: the buying price, with domain {vhigh, high, med, low}.
   MAINT: the maintenance price, with domain {vhigh, high, med, low}.
   DOORS: the number of doors, with domain {2, 3, 4, 5more}.
   PERSONS: the capacity in terms of persons to carry, with domain {2, 4, more}.
   LOG_BOOT: the size of luggage boot, with domain {small, med, big}.
   SAFETY: the estimated safety of the car, with domain {low, med, high}.
   The target attribute, EVALUATION, provides a description of the car's value as 
   unacceptable (unacc), acceptable (acc), good (good), or very good (vgood).

   Overall, your task is to build a Naive Bayes Classifier.

   Specifically, you need to:

   1. General and store all the probabilities that will be used by your Naive Bayes Classifier 
   based on the given data in CarData.csv. If any probability is 0, use Laplace smoothing 
   algorithm wih k=2 to smooth the noises. (You can use Your Lab 4 program and result.)
   
   2. Design and implement the Naive Bayes Classifier, and generate the class labels for each 
   data item that's in the training data set CarData.csv. 
   Save your predication result to a csv or text file.
   
   3. Implement a program to compare the result generated by your Naive Bayes Classfier and 
   the given result, and fill in a table with corresponding counts or percentages for 
   all the Evaluation attribute values. 

## How to compile and run the program
   To run the program and populate the tables, please enter:
      'python3 A3.py CarData.csv'
   or substitute whatever path locations are needed for each data set.

## File Contents
   probTables.txt contains all probability tables
   predTable.txt contains predicted Evaluation values
   accTable.txt contains models performace, counts of actual and predicted value results

## How the Probability Tables were calculated
   LaPlace Smoothing Algorithm was used to calculate the probabilities.
   Counts were done of all items, to see how many items fell into each attribute value, target
   value pair. There were many categories with 0 counts, which would have ruined our predictions.
   To combate this, LaPlace Smoothing Algorithm was used to calculate the probabilities for each
   cell in the probability tables. 

      LaPlace Smoothing Algorithm
         For each value Vi that each Attribute Ai can take on, along with each Cn attributes that
         Target can take on, we apply this formula to build the tables for each Attribute.

         Let numRowCol be the number of items in dataset with Ai = Vi AND EVAL = Cj
         Let numCol be the number of items in dataset with EVAL = Ci
         Let numRow be the number of Values which Ai can take on
         k = 2 

         The cell in the probability table corresponding with Ai = Vi and T = Cj: 
            Probability = ((numRowCol + k) / (numCol + (numRow*k)))
      
   To calculate the probability of each Ci which Eval can take on:
      
      Probability = numCi / totalNumDataItems

   Probability tables produced can be found in probTables.txt

## How the Evalutation Predictions were calculated
  
   For each item in the dataset, a probability was calculated for each Ci which Target 
   Attribute Evaluation can take on. 
   Then, the predicted Ci for that item was the Ci associated with the highest probability.
   Naive Bayes Classifier was used to caluclate the probabilities used to predict the evaluation classification for each data item.
      
      Naive Bayes Algorithm
         To calculate the probability that the data item takes on some classification value Cj, which
         Target Attribute 'Evaluation' can take on:
            Let M be the number of data items.
            Let P(Ai = Vi | t = Cj) be the probability an item with classification Ci will have 
               Attribute i equal to Value i
            Let P(t = Cj) be the probability an item will have classification Cj
            Then, 
            Probability = (multiply all P(Ai = Vi | t = Cj) for i = 1 up to M ) * P(t = Cj)

            This Probability is calculated for all Cj with Target Attribute can take one.
            The highest probability is the predicted target value.

   Predictions can be seen in predTable.txt

   After all predictions had been made, the actual results were compared to the predicted results.
   Counts were made of all pairs of target attributes values.
   These counts were outputted to an accuracy table which can be found in accTable.txt


## Notes 
   The accuracy table shows that this model has good results when there was many items of a 
   certain Evaluation category (e.g. unacc). 
   Where there was smaller counts (e.g. good, vgood which accounted for roughly 4% of the data 
   each) the accuracy was very bad. More predictions were wrong than were right. 
   I suspect this is due to the Laplace Smoothing too much, when there was not enough data items
   which fell into these categories.
   When testing other k values, the higher the k, the worse the predictions are for good and 
   vgood, and slightly less performance for acc. But unacc predicitions did not change as 
   much even for higher k values.
   A k value of 1 produced slightly better predictions for vgood (just over half correct),
   but barely any changes were seen in the predictions for good, acc or unacc.
   I believe more data is needed to build a more robhust prediction model.
                              